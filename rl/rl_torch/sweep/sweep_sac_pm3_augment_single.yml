program: rl/rl_torch/sac.py
method: grid
metric:
  name: reward/train
  goal: maximize
parameters:
  env_cfg.env_name:
    values: ["pointmass3d_augment"]
  env_cfg.seed:
    values: [123, 456, 789, 321, 654]
  env_cfg.total_episodes:
    values: [50]
  env_cfg.eval_interval:
    values: [5]
  env_cfg.log_interval:
    values: [5]
  env_cfg.num_envs:
    values: [200]
  env_cfg.random_robot_state:
    values: [True]
  env_cfg.random_target_state:
    values: [True]
  env_cfg.single_task:
    values: [True]
  agent_cfg.name:
    values: ["sac"]
  agent_cfg.lr:
    values: [0.0018]
  agent_cfg.policy_lr:
    values: [0.0023]
  agent_cfg.entropy_tuning:
    values: [True]
  agent_cfg.value_net_kwargs.layernorm:
    values: [True]
  agent_cfg.value_net_kwargs.droprate:
    values: [0.01]
  agent_cfg.value_net_kwargs.activation:
    values: ["selu"]
  buffer_cfg.multi_step:
    values: [3]
  buffer_cfg.prioritize_replay:
    values: [True]

