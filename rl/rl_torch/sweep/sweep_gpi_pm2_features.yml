program: rl/rl_torch/compose.py
method: grid
metric:
  name: reward/train
  goal: maximize
parameters:
  env_cfg.env_name:
    values: ["pointmass2d", "pointmass2d_simple", "pointmass2d_augment"]
  env_cfg.seed:
    values: [123, 456, 789]
  env_cfg.total_episodes:
    values: [20]
  env_cfg.eval_interval:
    values: [2]
  env_cfg.log_interval:
    values: [2]
  env_cfg.num_envs:
    values: [200]
  env_cfg.random_robot_state:
    values: [True]
  env_cfg.random_target_state:
    values: [True]
  env_cfg.single_task:
    values: [False]
  agent_cfg.name:
    values: ["sfgpi"]
  agent_cfg.lr:
    values: [0.005798]
  agent_cfg.policy_lr:
    values: [0.006056]
  agent_cfg.is_clip_max:
    values: [1]
  agent_cfg.entropy_tuning:
    values: [True]
  agent_cfg.value_net_kwargs.layernorm:
    values: [False]
  agent_cfg.value_net_kwargs.droprate:
    values: [0.0]
  agent_cfg.value_net_kwargs.activation:
    values: ["relu"]
  buffer_cfg.multi_step:
    values: [3]
  buffer_cfg.prioritize_replay:
    values: [True]





