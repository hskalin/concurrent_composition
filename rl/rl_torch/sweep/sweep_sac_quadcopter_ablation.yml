program: rl/rl_torch/sac.py
method: bayes
metric:
  name: reward/train
  goal: maximize
parameters:
  env_cfg.env_name:
    values: ["quadcopter_task0"]
  env_cfg.seed:
    values: [123, 456, 789]
  env_cfg.total_episodes:
    values: [200]
  env_cfg.eval_interval:
    values: [20]
  env_cfg.log_interval:
    values: [20]
  env_cfg.num_envs:
    values: [200]
  env_cfg.random_robot_state:
    values: [True]
  env_cfg.random_target_state:
    values: [True]
  agent_cfg.name:
    values: ["sac"]
  agent_cfg.lr:
    min: 0.0001
    max: 0.01
  agent_cfg.policy_lr:
    min: 0.0001
    max: 0.01
  agent_cfg.entropy_tuning:
    values: [True, False]
  agent_cfg.value_net_kwargs.layernorm:
    values: [True, False]
  agent_cfg.value_net_kwargs.droprate:
    values: [0.0, 0.005, 0.01]
  agent_cfg.value_net_kwargs.activation:
    values: ["selu", "relu"]
  buffer_cfg.multi_step:
    values: [3, 5, 7]
  buffer_cfg.prioritize_replay:
    values: [True, False]
